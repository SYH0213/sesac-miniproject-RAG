import gradio as gr
from openai import OpenAI
import os
from dotenv import load_dotenv
from llama_parse import LlamaParse

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore
from langchain.schema import Document
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# LLM ëª¨ë¸ ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# --- Document Loading and Caching ---
PDF_PATH = "data/gemini-2.5-tech_1-2.pdf"
PARSED_MD_PATH = "loaddata/llamaparse_output_gemini_1_2.md"
CHROMA_DB_DIR = "./chroma_db"

# --- RAG Setup ---

# 1. Text Splitters
parent_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=300)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)

# 2. Embedding Model
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# 3. Vector Store (ChromaDB)
vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings)

# 4. ParentDocumentRetriever
store = InMemoryStore()
retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=store,
    child_splitter=child_splitter,
    parent_splitter=parent_splitter,
)

# --- Data Loading and Vector Store Population ---
def load_and_populate_vectorstore():
    # Step 1: Check if the vector store is already populated.
    if vectorstore._collection.count() > 0:
        print(f"Vector store already populated with {vectorstore._collection.count()} documents. Skipping.")
        return

    # Step 2: If not populated, check for the parsed markdown file.
    if not os.path.exists(PARSED_MD_PATH):
        print(f"'{PARSED_MD_PATH}' not found. Processing PDF with LlamaParse...")
        api_key = os.getenv("LLAMA_CLOUD_API_KEY")
        if not api_key:
            print("Error: LLAMA_CLOUD_API_KEY not found.")
            return
        try:
            parser = LlamaParse(result_type="markdown", api_key=api_key)
            documents = parser.load_data(PDF_PATH)
            with open(PARSED_MD_PATH, "w", encoding="utf-8") as f:
                f.write("\n".join([doc.text for doc in documents]))
            print(f"Successfully parsed and saved to '{PARSED_MD_PATH}'")
        except Exception as e:
            print(f"LlamaParse processing error: {e}")
            return
    
    # Step 3: Load the document and add to the vector store.
    print(f"Loading document from '{PARSED_MD_PATH}'...")
    try:
        with open(PARSED_MD_PATH, "r", encoding="utf-8") as f:
            text = f.read()
        documents = [Document(page_content=text)]
        print("Adding documents to vector store...")
        retriever.add_documents(documents)
        print(f"Vector store populated with {vectorstore._collection.count()} documents.")
    except Exception as e:
        print(f"Error loading or processing markdown file: {e}")

# --- Conversational RAG Chain Setup ---

# 1. Prompt for History-Aware Retriever
contextualize_q_system_prompt = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is."
)
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)

# 2. Create the History-Aware Retriever
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

# 3. Prompt for Final Answer Generation
ga_system_prompt = (
    "You are a helpful assistant. Your ONLY task is to answer the user's question STRICTLY based on the provided context. "
    "If the information to answer the question is present in the context, provide a concise answer. "
    "If the answer cannot be found within the provided context, you MUST say 'ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ìœ¼ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' Do NOT use any of your outside knowledge."
    "\n\nContext:\n{context}"
)
ga_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", ga_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)

# 4. Create the Document Chain
question_answer_chain = create_stuff_documents_chain(llm, ga_prompt)

# --- RAG Function ---
def ask_llm(query, history):
    chat_history_for_chain = []
    if history:
        for message in history:
            if message["role"] == "user":
                chat_history_for_chain.append(HumanMessage(content=message["content"]))
            elif message["role"] == "assistant":
                chat_history_for_chain.append(AIMessage(content=message["content"]))

    try:
        # --- DEBUGGING STARTS HERE ---
        print("\n" + "="*50)
        print(f"DEBUG: Current Query: {query}")
        
        print("\n--- 1. Invoking History-Aware Retriever to get documents ---")
        retrieved_docs = history_aware_retriever.invoke({
            "input": query,
            "chat_history": chat_history_for_chain
        })
        print(f"\n--- 2. Retriever found {len(retrieved_docs)} documents. ---")
        if retrieved_docs:
            for i, doc in enumerate(retrieved_docs):
                source = doc.metadata.get('source', 'N/A') if hasattr(doc, 'metadata') else 'N/A'
                print(f"--- Document {i+1} (Source: {source}) ---")
                print(doc.page_content[:300] + "...")
                print("-"*(len(f"--- Document {i+1} (Source: {source}) ---")))
        print("\n" + "="*50)

        print("\n--- 3. Invoking Document Chain to generate answer ---")
        final_input = {
            "input": query,
            "chat_history": chat_history_for_chain,
            "context": retrieved_docs
        }
        answer = question_answer_chain.invoke(final_input)
        print(f"\n--- 4. Final Answer Generated ---")
        print(answer)
        print("="*50 + "\n")
        # --- DEBUGGING ENDS HERE ---

        context_text = "## ì°¸ì¡° ë¬¸ì„œ\n\n"
        if retrieved_docs:
            for i, doc in enumerate(retrieved_docs):
                context_text += f"### ë¬¸ì„œ {i+1}\n"
                context_text += f"```\n{doc.page_content}\n```\n\n"
        else:
            context_text += "ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

        if not history:
            history = []
        history.append({"role": "user", "content": query})
        history.append({"role": "assistant", "content": answer})
        
        return "", history, context_text
        
    except Exception as e:
        error_message = f"ì˜¤ë¥˜ ë°œìƒ: {e}"
        debug_info = f"\n\nDEBUG INFO:\nQuery: {query}\nHistory: {history}"
        full_error = f"{error_message}{debug_info}"
        print(f"ERROR in ask_llm: {full_error}")

        if not history:
            history = []
        history.append({"role": "user", "content": query})
        history.append({"role": "assistant", "content": error_message})
        return "", history, "ì°¸ì¡°ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

# --- Vector Store Management Function ---
def force_reload_vectorstore():
    print("---" + " Forcing reload of vector store ---")
    try:
        print("Attempting to reset the collection via chromadb client...")
        vectorstore._client.reset()
        print("Successfully reset the chromadb client.")
        
        load_and_populate_vectorstore()
        return "âœ… Vector store reloaded successfully!"
    except Exception as e:
        error_msg = f"âŒ Error during vector store reload: {e}"
        print(error_msg)
        return error_msg

# --- Gradio Interface ---
load_and_populate_vectorstore()

example_questions_doc_content = [
    "Gemini 2.5ëŠ” ì–´ë–¤ ëª¨ë¸ ê³„ì—´ë¡œ ì„¤ëª…ë˜ê³  ìˆë‚˜ìš”?",
    "ë¬¸ì„œì—ì„œ ê°•ì¡°í•˜ëŠ” Gemini 2.5ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
    "Gemini 2.5ëŠ” ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ê°œì„ ë˜ì—ˆë‹¤ê³  í•˜ë‚˜ìš”?"
]

example_questions_excel = [
    "Gemini 1.5 Proì™€ Gemini 2.5 ProëŠ” ì…ë ¥ ê¸¸ì´ì™€ ì¶œë ¥ ê¸¸ì´ì—ì„œ ì–´ë–¤ ì°¨ì´ê°€ ìˆë‚˜ìš”?",
    "Gemini 2.0 Flashì™€ Gemini 2.5 Flashì˜ ì¶œë ¥ ëª¨ë‹¬ë¦¬í‹° ì°¨ì´ëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
    "Thinking ê¸°ëŠ¥ì´ ì—†ëŠ” ëª¨ë¸ê³¼ ìˆëŠ” ëª¨ë¸ì€ ê°ê° ë¬´ì—‡ì¸ê°€ìš”?",
    "Knowledge cutoff ê¸°ì¤€ìœ¼ë¡œ Gemini 1.5ì™€ Gemini 2.5ëŠ” ê°ê° ì–¸ì œê¹Œì§€ì˜ ì •ë³´ë¥¼ ë°˜ì˜í•˜ë‚˜ìš”?",
    "Gemini 2.0 Flash-Liteì™€ Gemini 2.5 ProëŠ” ì–´ë–¤ ì ì—ì„œ ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì´ë‚˜ìš”?"
]

with gr.Blocks(theme="soft", title="PDF RAG Chatbot") as demo:
    gr.Markdown("# PDF RAG Chatbot (LlamaParse + Conversational)")
    gr.Markdown("PDF ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”. (ëŒ€í™” ë‚´ìš© ê¸°ì–µ ê¸°ëŠ¥ í¬í•¨)")
    
    with gr.Row():
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(height=400, label="Chat", type='messages', value=[])
            msg = gr.Textbox(label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
            
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ë¬¸ì„œ ë‚´ìš© í™•ì¸ìš©")
                    gr.Examples(
                        examples=example_questions_doc_content,
                        inputs=msg,
                        label="ë¬¸ì„œ ë‚´ìš© í™•ì¸ìš© ì§ˆë¬¸"
                    )
                with gr.Column():
                    gr.Markdown("### ì—‘ì…€í‘œ ë¡œë“œ í™•ì¸ìš©")
                    gr.Examples(
                        examples=example_questions_excel,
                        inputs=msg,
                        label="ì—‘ì…€í‘œ ë¡œë“œ í™•ì¸ìš© ì§ˆë¬¸"
                    )
        with gr.Column(scale=1):
            context_display = gr.Markdown(label="LLM ì°¸ì¡° ë¬¸ì„œ ì „ë¬¸")
            with gr.Accordion("âš™ï¸ Advanced Options", open=False):
                reload_button = gr.Button("ğŸ”„ Force Reload Vector Store")
                reload_status = gr.Markdown()

    clear = gr.ClearButton([msg, chatbot, context_display])
    msg.submit(ask_llm, [msg, chatbot], [msg, chatbot, context_display])
    reload_button.click(force_reload_vectorstore, outputs=reload_status)

demo.launch()